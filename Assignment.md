



# PE Dashboard AI50 - Automated Private Equity Intelligence

**Project ORBIT** - Forbes AI 50 PE Dashboard Factory

Automated system for generating investment-grade PE dashboards for Forbes AI 50 companies using dual LLM pipelines with real-time GCS streaming.

---
## üìä Current StatusThis starts:

- FastAPI: http://localhost:8000
- Codelab Link: https://codelabs-preview.appspot.com/?file_id=1q55eKm20EeYkN_g0Q4KdVb7A7g28D87yIkz8lyX2p8M#4
- demo video:- https://youtu.be/MvWR_xlf49E

## üéØ Project Overview

This project automates the generation of investment-grade dashboards for the Forbes AI 50 companies, replacing manual analyst workflows with an AI-powered pipeline that:

- ‚úÖ **Ingests** public data from company websites, career pages, and news sources (via Airflow)
- ‚úÖ **Stores** all data in Google Cloud Storage (single source of truth)
- ‚úÖ **Streams** data on-demand from GCS (no local caching)
- ‚úÖ **Generates** comprehensive PE dashboards using two approaches:
  - **Structured Pipeline**: GCS Payload ‚Üí Pydantic ‚Üí ChatGPT ‚Üí Dashboard
  - **RAG Pipeline**: GCS ChromaDB ‚Üí Semantic Search ‚Üí ChatGPT ‚Üí Dashboard
- ‚úÖ **Updates** data daily via Airflow DAGs (3 AM schedule)
- ‚úÖ **Serves** dashboards via FastAPI + Streamlit UI

---

## üèóÔ∏è Architecture

### Complete System Flow with GCS Streaming

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               AIRFLOW ORCHESTRATION (Daily Updates)             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  Scrape Jobs ‚îÇ ‚Üí ‚îÇ Extract Data ‚îÇ ‚Üí ‚îÇ  Update GCS  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  (3 AM Daily)‚îÇ    ‚îÇ   & Process  ‚îÇ    ‚îÇ   Bucket     ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            GOOGLE CLOUD STORAGE (Source of Truth)               ‚îÇ
‚îÇ  gs://us-central1-pe-airflow-env-2825d831-bucket/              ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ data/payloads/       (48 companies - Pydantic JSON)       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ data/vector_db/      (ChromaDB - 215 chunks)              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ data/jobs/           (45 companies - Hiring data)         ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚úÖ ALWAYS LATEST DATA - Updated daily by Airflow               ‚îÇ
‚îÇ  ‚úÖ NO LOCAL CACHING - Streamed on-demand                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              FASTAPI + STREAMLIT (Query Interface)              ‚îÇ
‚îÇ  User Request ‚Üí API Endpoint ‚Üí Stream from GCS ‚Üí Generate       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 DUAL PIPELINE GENERATION (Real-time)            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   STRUCTURED PIPELINE      ‚îÇ  ‚îÇ      RAG PIPELINE        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                            ‚îÇ  ‚îÇ                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  1. Stream payload.json    ‚îÇ  ‚îÇ  1. Query GCS ChromaDB   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     from GCS               ‚îÇ  ‚îÇ     (semantic search)    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  2. Validate with Pydantic ‚îÇ  ‚îÇ  2. Retrieve top-k       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  3. Format as JSON context ‚îÇ  ‚îÇ     chunks               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  4. Call ChatGPT           ‚îÇ  ‚îÇ  3. Assemble context     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     (gpt-4o-mini)          ‚îÇ  ‚îÇ  4. Call ChatGPT         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  5. Generate dashboard     ‚îÇ  ‚îÇ     (gpt-4o-mini)        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                            ‚îÇ  ‚îÇ  5. Generate dashboard   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚ö° Uses latest GCS data   ‚îÇ  ‚îÇ  ‚ö° Uses latest GCS data  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DASHBOARD OUTPUT (Generated On-The-Fly)            ‚îÇ
‚îÇ  ‚úÖ 8 Required Sections:                                        ‚îÇ
‚îÇ  1. Company Overview                                            ‚îÇ
‚îÇ  2. Business Model and GTM                                      ‚îÇ
‚îÇ  3. Funding & Investor Profile                                  ‚îÇ
‚îÇ  4. Growth Momentum (with latest jobs data from GCS)            ‚îÇ
‚îÇ  5. Visibility & Market Sentiment                               ‚îÇ
‚îÇ  6. Risks and Challenges                                        ‚îÇ
‚îÇ  7. Outlook                                                     ‚îÇ
‚îÇ  8. Disclosure Gaps                                             ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  üìä Always reflects LATEST data from GCS (updated daily)        ‚îÇ
‚îÇ  üö´ NO pre-generated files - everything on-demand               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Architecture Principles

1. **Single Source of Truth**: GCS bucket contains ALL data
2. **No Local Storage**: API streams data directly from GCS (no downloads)
3. **Always Fresh**: Airflow updates GCS daily at 3 AM
4. **On-Demand Generation**: Dashboards created in real-time per request
5. **Dual Validation**: Both Pydantic (structured) and ChromaDB (RAG) ensure quality

---

## üìä Current Status

### Data Coverage (in GCS Bucket)
- **50 Companies**: All Forbes AI 50 companies indexed
- **48 Structured Payloads**: Validated Pydantic schemas in GCS (`data/payloads/`)
- **215 ChromaDB Chunks**: Vector database in GCS (`data/vector_db/`)
- **45 Jobs Files**: Hiring data in GCS (`data/jobs/`)
- **Daily Updates**: Airflow refreshes data every day at 3 AM

### System Capabilities
- **Real-time Generation**: Dashboards created on-the-fly from GCS
- **Always Fresh**: No stale data - streams latest from cloud
- **Dual Pipeline**: Structured (Pydantic) + RAG (ChromaDB)
- **Format**: Bloomberg Terminal-style (500-1500 words)
- **Quality**: Both pipelines score 8.0-8.1/10 average

### Technical Stack
- **LLM**: OpenAI GPT-4o-mini (fast, cost-effective)
- **Vector DB**: ChromaDB with GCS persistence
- **Validation**: Pydantic 2.x for type safety
- **Storage**: Google Cloud Storage (no local caching)
- **Orchestration**: Apache Airflow (daily updates)
- **API**: FastAPI + Uvicorn
- **UI**: Streamlit multi-page app

---

## üöÄ Quick Start

### Prerequisites
- Python 3.11+
- Docker Desktop (for containerized deployment)
- OpenAI API Key
- Google Cloud credentials (for GCS access)

### Installation

```bash
# Clone repository
git clone https://github.com/Big-Data-IA-Team-5/pe-dashboard-ai50.git
cd pe-dashboard-ai50

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env and add:
#   OPENAI_API_KEY=sk-...
#   GCS_BUCKET_NAME=us-central1-pe-airflow-env-2825d831-bucket
#   GOOGLE_APPLICATION_CREDENTIALS=./gcp-service-account-key.json
```

### Local Development

#### Option 1: Run Locally (Recommended for Development)

```bash
# Terminal 1: Start FastAPI backend
uvicorn src.api:app --reload --port 8000

# Terminal 2: Start Streamlit frontend
streamlit run src.streamlit_app.py --server.port 8501
```

Access:
- **API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs
- **Streamlit UI**: http://localhost:8501

#### Option 2: Run with Docker (Recommended for Production)

```bash
cd docker
docker compose up --build -d
```

Access:
- **API**: http://localhost:8000
- **Streamlit UI**: http://localhost:8501

Check status:
```bash
docker compose ps
docker compose logs -f api        # View API logs
docker compose logs -f streamlit  # View Streamlit logs
```

---

## üìñ Usage Guide

### Generate Dashboard (API)

**Structured Pipeline** (streams from GCS):
```bash
curl -X POST "http://localhost:8000/dashboard/structured?company_id=anthropic&use_gcs=true"
```

**RAG Pipeline** (queries GCS ChromaDB):
```bash
curl -X POST "http://localhost:8000/dashboard/rag?company_id=anthropic&use_gcs=true&top_k=10"
```

**Compare Both** (side-by-side):
```bash
curl -X POST "http://localhost:8000/compare?company_id=anthropic"
```

### Generate Dashboard (Streamlit UI)

1. Open http://localhost:8501
2. Select a company from dropdown (50 companies available)
3. Click "Generate (Structured)" or "Generate (RAG)"
4. View dashboard with metadata and validation results
5. Download as Markdown file

### Key API Parameters

- `company_id`: Company identifier (e.g., `anthropic`, `openai`)
- `use_gcs`: Stream from GCS (default: `true`) or use local fallback
- `top_k`: Number of chunks for RAG (default: 10)

---

## üìÅ Project Structure

```
pe-dashboard-ai50/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api.py                    # FastAPI backend (8 endpoints)
‚îÇ   ‚îú‚îÄ‚îÄ streamlit_app.py          # Streamlit UI (4 tabs)
‚îÇ   ‚îú‚îÄ‚îÄ structured_pipeline.py    # GCS Payload ‚Üí Pydantic ‚Üí LLM
‚îÇ   ‚îú‚îÄ‚îÄ rag_pipeline.py           # GCS ChromaDB ‚Üí Retrieval ‚Üí LLM
‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py             # OpenAI wrapper (gpt-4o-mini)
‚îÇ   ‚îú‚îÄ‚îÄ chromadb_gcs.py           # ChromaDB with GCS persistence
‚îÇ   ‚îú‚îÄ‚îÄ gcs_client.py             # GCS streaming client (singleton)
‚îÇ   ‚îú‚îÄ‚îÄ jobs_loader.py            # Jobs data loader
‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py              # Dashboard comparison rubric
‚îÇ   ‚îî‚îÄ‚îÄ models.py                 # Pydantic schemas (DO NOT MODIFY)
‚îÇ
‚îú‚îÄ‚îÄ data/                         # LOCAL COPY (for reference only)
‚îÇ   ‚îú‚îÄ‚îÄ forbes_ai50_seed.json     # All 50 companies metadata
‚îÇ   ‚îú‚îÄ‚îÄ payloads/                 # 48 structured payloads
‚îÇ   ‚îú‚îÄ‚îÄ vector_db/                # ChromaDB (215 chunks)
‚îÇ   ‚îî‚îÄ‚îÄ jobs/                     # 45 companies with hiring data
‚îÇ
‚îú‚îÄ‚îÄ dags/                         # Airflow DAGs (orchestration)
‚îÇ   ‚îú‚îÄ‚îÄ ai50_full_ingest_dag.py   # Full ingestion pipeline
‚îÇ   ‚îî‚îÄ‚îÄ ai50_daily_refresh_dag.py # Daily updates (3 AM schedule)
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                # Multi-service container
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml        # API + Streamlit services
‚îÇ
‚îú‚îÄ‚îÄ PE_Dashboard_Concise.md       # System prompt (Bloomberg style)
‚îú‚îÄ‚îÄ gcp-service-account-key.json  # GCS credentials (DO NOT COMMIT)
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îî‚îÄ‚îÄ README.md                     # This file
```

---

## üîß Configuration

### Environment Variables (.env)

```bash
# Required
OPENAI_API_KEY=sk-...                                          # OpenAI API key
GCS_BUCKET_NAME=us-central1-pe-airflow-env-2825d831-bucket   # GCS bucket
GOOGLE_APPLICATION_CREDENTIALS=./gcp-service-account-key.json # GCS auth

# Optional
API_BASE_URL=http://localhost:8000                            # API endpoint
```

---

## üìä Dashboard Format

All dashboards follow an 8-section structure with strict validation:

### 1. Company Overview
- Company name, location, founding year
- Leadership team (CEO, CTO, executives)
- Industry category

### 2. Business Model and GTM
- Target customers and market segments
- Product lineup with descriptions
- Pricing models (if disclosed)

### 3. Funding & Investor Profile
- Funding history with dates and amounts
- Lead investors and syndicate members
- Last valuation and total capital raised

### 4. Growth Momentum
- Current headcount and growth rate
- **Open job positions** (engineering, sales, other) - **STREAMED FROM GCS DAILY**
- Recent product launches

### 5. Visibility & Market Sentiment
- News mentions (last 30 days)
- Average sentiment score
- GitHub stars and Glassdoor ratings

### 6. Risks and Challenges
- Competitive pressure
- Regulatory risks
- Market uncertainties

### 7. Outlook
- Growth trajectory assessment
- Market fit evaluation

### 8. Disclosure Gaps
- **Critical**: Lists ALL missing information
- Examples: "Valuation not disclosed", "Customer counts not disclosed"

---

## üî¨ API Endpoints

### Core Dashboard Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check + pipeline status |
| `/companies` | GET | List all 50 companies with payload availability |
| `/dashboard/structured` | POST | Generate structured dashboard (GCS streaming) |
| `/dashboard/rag` | POST | Generate RAG dashboard (GCS ChromaDB) |
| `/compare` | POST | Side-by-side comparison of both pipelines |
| `/company/{id}/metadata` | GET | Data quality metrics |

### Example Request/Response

```bash
# Request
curl -X POST "http://localhost:8000/dashboard/structured?company_id=anthropic&use_gcs=true"

# Response
{
  "company_id": "anthropic",
  "method": "structured_gcs",
  "markdown": "# Private Equity Dashboard for Anthropic\n\n## Company Overview...",
  "validation": {
    "valid": true,
    "section_count": 8,
    "present_sections": ["## Company Overview", ...]
  },
  "metadata": {
    "company_name": "Anthropic",
    "pipeline": "structured_gcs",
    "model": "gpt-4o-mini",
    "data_source": "gcs",
    "response_time_seconds": 11.88,
    "num_events": 1,
    "num_products": 4
  }
}
```

---

## üõ†Ô∏è Development

### Test API

```bash
# Health check
curl http://localhost:8000/health

# List companies
curl http://localhost:8000/companies | python3 -m json.tool | head -30

# Generate test dashboard (GCS streaming)
curl -X POST "http://localhost:8000/dashboard/structured?company_id=anthropic&use_gcs=true" \
  | python3 -m json.tool

# Test RAG pipeline (GCS ChromaDB)
curl -X POST "http://localhost:8000/dashboard/rag?company_id=anthropic&use_gcs=true&top_k=10"
```

---

## üöß Troubleshooting

### Issue: "ChromaDB not found"
```bash
pip install chromadb>=0.4.22
```

### Issue: "OpenAI API key invalid"
```bash
# Check .env file
cat .env | grep OPENAI_API_KEY

# Verify API key at: https://platform.openai.com/api-keys
```

### Issue: "GCS authentication failed"
```bash
# Verify service account key exists
ls -la gcp-service-account-key.json

# Test GCS access
gsutil ls gs://us-central1-pe-airflow-env-2825d831-bucket/data/
```

### Issue: "Docker containers not starting"
```bash
# Check Docker is running
docker ps

# View logs
docker compose logs -f

# Rebuild from scratch (no cache)
docker compose down
docker compose build --no-cache
docker compose up -d
```

---

## üìù Assignment Context

**Course**: DAMG7245 - Big Data Systems and Intelligence Analytics  
**Assignment**: Assignment 2 - Project ORBIT (Part 1)  
**Team**: Big-Data-IA-Team-5  

### Key Deliverables
- ‚úÖ Working FastAPI with 8 endpoints (GCS streaming)
- ‚úÖ Working Streamlit UI with 4 tabs
- ‚úÖ Two dashboard pipelines (Structured + RAG)
- ‚úÖ Real-time GCS data streaming (no local caching)
- ‚úÖ Daily Airflow updates (3 AM schedule)
- ‚úÖ Docker deployment (docker-compose.yml)
- ‚úÖ Vector database (215 chunks in GCS)
- ‚úÖ On-demand dashboard generation

### Critical Rules
1. **Never invent data**: Use "Not disclosed" for missing information
2. **Fixed schema**: 8 sections required in exact order
3. **Provenance**: Every claim must be traceable to sources
4. **No hallucination**: No speculative language ("likely", "estimated")
5. **Always fresh**: All data streamed from GCS (updated daily)

### How It Works

#### Structured Pipeline
```
GCS Payload (JSON) ‚Üí Pydantic Validation ‚Üí ChatGPT ‚Üí Dashboard
```
- **Purpose**: Use Pydantic for data validation, ChatGPT for text generation
- **Input**: Structured JSON from GCS
- **Output**: Human-readable markdown dashboard

#### RAG Pipeline
```
GCS ChromaDB ‚Üí Semantic Search ‚Üí ChatGPT ‚Üí Dashboard
```
- **Purpose**: Use ChromaDB for retrieval, ChatGPT for synthesis
- **Input**: Vector database queries from GCS
- **Output**: Context-enriched markdown dashboard

---

## üìÑ License

MIT License - See LICENSE file

---

## üôè Acknowledgments

- **Forbes AI 50**: Source data
- **OpenAI**: GPT-4o-mini model
- **ChromaDB**: Vector database
- **Google Cloud**: Storage and infrastructure

---

**Last Updated**: November 8, 2025  
**Version**: 3.0.0  
**Status**: Production Ready with GCS Streaming ‚úÖ
